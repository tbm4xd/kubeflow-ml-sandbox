{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End to end experiment: Github Issue Summarization\n",
    "\n",
    "Currently, this notebook must be run from the Kubeflow JupyterHub installation, as described in the codelab.\n",
    "\n",
    "In this notebook, we will show how to:\n",
    "\n",
    "* Interactively define a KubeFlow Pipeline using the Pipelines Python SDK\n",
    "* Submit and run the pipeline\n",
    "* Add a step in the pipeline\n",
    "\n",
    "This example pipeline trains a [Tensor2Tensor](https://github.com/tensorflow/tensor2tensor/) model on Github issue data, learning to predict issue titles from issue bodies. It then exports the trained model and deploys the exported model to [Tensorflow Serving](https://github.com/tensorflow/serving). \n",
    "The final step in the pipeline launches a web app which interacts with the TF-Serving instance in order to get model predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enviroinment Setup\n",
    "\n",
    "Before any experiment can be conducted. We need to setup and initialize an environment: ensure all Python modules has been setup and configured, as well as python modules\n",
    "\n",
    "### Imports\n",
    "Setting up python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%reload_ext nbextensions\n",
    "%load_nbvars\n",
    "\n",
    "import kfp.dsl as dsl\n",
    "import kfp.gcp as gcp\n",
    "import pandas as pd\n",
    "from ipython_secrets import get_secret\n",
    "from kfp.compiler import Compiler\n",
    "from kfp.components import load_component_from_file\n",
    "from os import environ\n",
    "import boto3, kfp\n",
    "\n",
    "from nbextensions.pv import use_pvc\n",
    "from nbextensions.kubernetes import use_pull_secret\n",
    "from nbextensions.aws import upload_to_s3\n",
    "\n",
    "import nbextensions.utils as utils\n",
    "from datetime import datetime\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define global variables\n",
    "\n",
    "Initialize global namespace variables. It is a good practice to place all global namespace variables in one cell. So, the notebook could be configured all-at-once. \n",
    "\n",
    "To enhance readability we would advice to capitalize such variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "USER = environ.get('NB_USER', 'John Doe')\n",
    "TAG = 'latest'\n",
    "# TAG = 'v8'\n",
    "\n",
    "BUILD_CONTEXT = f\"{TAG}/buildcontext\"\n",
    "TRAINING_IMAGE = f\"{DOCKER_REGISTRY}/training:{TAG}\"\n",
    "SERVING_IMAGE = f\"{DOCKER_REGISTRY}/seldon:{TAG}\"\n",
    "FLASK_APP_IMAGE = f\"{DOCKER_REGISTRY}/flask:{TAG}\"\n",
    "TRAINING_ROOT = f\"{MOUNT_PATH}/{TAG}/training\"\n",
    "\n",
    "DATASET_FILE = f\"{TRAINING_ROOT}/dataset.csv\"\n",
    "MODEL_FILE = f\"{TRAINING_ROOT}/training1.h5\"\n",
    "TITLE_PP_FILE = f\"{TRAINING_ROOT}/title_preprocessor.dpkl\"\n",
    "BODY_PP_FILE = f\"{TRAINING_ROOT}/body_preprocessor.dpkl\"\n",
    "TRAIN_DF_FILE = f\"{TRAINING_ROOT}/traindf.csv\"\n",
    "TEST_DF_FILE =  f\"{TRAINING_ROOT}/testdf.csv\"\n",
    "TRAIN_TITLE_VECS = f\"{TRAINING_ROOT}/train_title_vecs.npy\"\n",
    "TRAIN_BODY_VECS = f\"{TRAINING_ROOT}/train_body_vecs.npy\"\n",
    "\n",
    "s3 = boto3.session.Session().client('s3', endpoint_url=BUCKET_ENDPOINT)\n",
    "\n",
    "client = kfp.Client()\n",
    "try:\n",
    "    exp = client.get_experiment(experiment_name=APPLICATION_NAME)\n",
    "except:\n",
    "    exp = client.create_experiment(APPLICATION_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define build docker image pipeline\n",
    "\n",
    "Define build pipeline. Yes, we arguably using KFP to build images  that will be de-facto used by final pipeline.\n",
    "\n",
    "We use [Kaniko](https://github.com/GoogleContainerTools/kaniko) and Kubernetes to handle build operations. Build status can be tracked via KFP pipeline dashboard\n",
    "\n",
    "In fact build image job can be even combined with primary pipeline as physically it will be different Kubernetes pods. However for sake of general purpose efficiency we schedule build process via separate pipeline step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaniko_op = load_component_from_file('components/kaniko/deploy.yaml')\n",
    "\n",
    "@dsl.pipeline(\n",
    "  name='Pipeline images',\n",
    "  description='Build images that will be used by the pipeline'\n",
    ")\n",
    "def build_image(\n",
    "        image, \n",
    "        build_context=None, \n",
    "        dockerfile: dsl.PipelineParam=dsl.PipelineParam(name='dockerfile', value='Dockerfile')):\n",
    "    kaniko_op(\n",
    "        image=image,\n",
    "        dockerfile=dockerfile,\n",
    "        build_context=build_context\n",
    "    ).apply(\n",
    "        # docker registry credentials \n",
    "        use_pull_secret(secret_name=DOCKER_REGISTRY_PULL_SECRET)\n",
    "    ).apply(\n",
    "        # s3 bucket volume clame has been injected here        \n",
    "        use_pvc(name=BUCKET_PVC, mount_to=MOUNT_PATH)\n",
    "    )\n",
    "        \n",
    "Compiler().compile(build_image, '.kaniko.tar.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compiler transforms Python DSL into an [Argo Workflow](https://argoproj.github.io/docs/argo/readme.html). And stores generated artifacts in [`.kaniko.tar.gz`](.kaniko.tar.gz). So it could be executed multiple times. Perhaps with different parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Training\n",
    "Training is an integral part of our experiment. Distributed training means that it will be executed outside of a Jupyter Notebook and utilize maximum capacity of the current cluster. To achieve this we need to perform following actions:\n",
    "* Build a dokcer image for training\n",
    "* Define a training pipeline\n",
    "* Run the experiment\n",
    "\n",
    "### Building a training image\n",
    "Once pipeline has been defined we can reuse it multiple times by supplying different input parameters.\n",
    "\n",
    "Next section will upload all files to s3, to share access with the pipeline. Files that should be ignored can be customized in [kanikoignore.txt](./kanikoignore.txt). To understand upload scenario you can review and modify: [aws.py](./extensions/kaniko/aws.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_to_s3(\n",
    "    destination=f\"s3://{BUCKET_NAME}/{BUILD_CONTEXT}\",\n",
    "    ignorefile='components/kaniko/ignorefile.txt',\n",
    "    workspace='.',\n",
    "    s3_client=s3\n",
    ")\n",
    "\n",
    "run = client.run_pipeline(\n",
    "    exp.id, f'Build image: training:{TAG}', '.kaniko.tar.gz', \n",
    "    params={\n",
    "        'image': TRAINING_IMAGE,\n",
    "        'build-context': f\"{MOUNT_PATH}/{BUILD_CONTEXT}/components/training\"\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build process can be long a long term. Because often images that has been used for data science tasks are huge. In this case you might want to adjust `timeout` parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# block until job completion\n",
    "print(f\"Waiting for run: {run.id}...\")\n",
    "result = client.wait_for_run_completion(run.id, timeout=720).run.status\n",
    "print(f\"Finished with: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Pipeline\n",
    "We have extracted code for training pipeline into a [component](components/training). Python code that defines `training_op` as well as a `http_download_op` can be found [here](components/training/component.py)\n",
    "\n",
    "Below we will define a pipeline that will run the training pipeline as an experimnet. This pipeline will do the following. Every training operation (except download) will be encapsulated by the python script. You can change the scripts at your will however, you will need to rebuild a training image.\n",
    "\n",
    "* Download dataset from http \n",
    "* Split data into sample and test. It can also put a rownum limit into a dataset to increase feedback\n",
    "* Preprocess data for machine learning (clean, tokenize and transform text into vector)\n",
    "* Apply sequence to sequence training with Keras. By the completion trained model will be uplooaded into s3 bucket "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from components.training import (http_download_op, training_op)\n",
    "\n",
    "@dsl.pipeline(\n",
    "  name='Training',\n",
    "  description=\"\"\"\n",
    "  Download dataset, \n",
    "  Split data set for training and validation, \n",
    "  Clean and preprocess data, \n",
    "  Train the model\n",
    "  \"\"\"\n",
    ")\n",
    "def training_pipeline(\n",
    "    import_from: dsl.PipelineParam, \n",
    "    dataset_file: dsl.PipelineParam,\n",
    "    dataset_md5: dsl.PipelineParam,\n",
    "    train_df_file: dsl.PipelineParam,\n",
    "    test_df_file: dsl.PipelineParam,\n",
    "    title_pp_file: dsl.PipelineParam,\n",
    "    body_pp_file: dsl.PipelineParam,\n",
    "    train_title_vecs: dsl.PipelineParam,\n",
    "    train_body_vecs: dsl.PipelineParam,\n",
    "    model_file: dsl.PipelineParam,\n",
    "    sample_size: dsl.PipelineParam=dsl.PipelineParam(name='sample-size', value='200'),\n",
    "    learning_rate: dsl.PipelineParam=dsl.PipelineParam(name='learning-rate', value=0.001),\n",
    "):  \n",
    "    download = http_download_op(\n",
    "        url=import_from,\n",
    "        md5sum=dataset_md5,\n",
    "        download_to=dataset_file\n",
    "    ).apply(\n",
    "        use_pvc(name=BUCKET_PVC, mount_to=MOUNT_PATH)\n",
    "    )\n",
    "    \n",
    "    # Generates the training and test set. Only processes \"sample-size\" rows.\n",
    "    process = training_op(\n",
    "        script='process_data.py',\n",
    "        arguments=[\n",
    "            '--input_csv', dataset_file,\n",
    "            '--sample_size', sample_size,\n",
    "            '--output_traindf_csv', train_df_file, \n",
    "            '--output_testdf_csv', test_df_file,\n",
    "        ]\n",
    "    ).apply(\n",
    "        use_pvc(name=BUCKET_PVC, mount_to=MOUNT_PATH)\n",
    "    ).after(download)\n",
    "    \n",
    "    # Preprocess for deep learning\n",
    "    preproc_for_ml = training_op(\n",
    "        script='preproc.py',\n",
    "        arguments=[\n",
    "            '--input_traindf_csv', train_df_file,\n",
    "            '--output_title_preprocessor_dpkl', title_pp_file,\n",
    "            '--output_body_preprocessor_dpkl', body_pp_file,\n",
    "            '--output_train_title_vecs_npy', train_title_vecs,\n",
    "            '--output_train_body_vecs_npy', train_body_vecs,\n",
    "        ],\n",
    "        file_outputs={\n",
    "            'title-example': '/tmp/train_title_raw.txt',\n",
    "            'title-processed': '/tmp/train_title_vecs.txt',\n",
    "            'body-example': '/tmp/train_body_raw.txt',\n",
    "            'body-processed': '/tmp/train_body_vecs.txt',\n",
    "        }\n",
    "    ).apply(\n",
    "        use_pvc(name=BUCKET_PVC, mount_to=MOUNT_PATH)\n",
    "    ).after(process)\n",
    "    \n",
    "    # Training\n",
    "    training = training_op(\n",
    "        script='train.py',\n",
    "        arguments=[\n",
    "            '--input_title_preprocessor_dpkl', title_pp_file,\n",
    "            '--input_body_preprocessor_dpkl', body_pp_file,\n",
    "            '--input_train_title_vecs_npy', train_title_vecs,\n",
    "            '--input_train_body_vecs_npy', train_body_vecs,\n",
    "            '--script_name_base', '/tmp/seq2seq',\n",
    "            '--output_model_h5', model_file,\n",
    "            '--learning_rate', learning_rate,\n",
    "           '--tempfile', \"True\",\n",
    "        ],\n",
    "        file_outputs={'train': '/tmp/seq2seq.log'},\n",
    "    ).apply(\n",
    "        use_pvc(name=BUCKET_PVC, mount_to=MOUNT_PATH)\n",
    "    ).after(preproc_for_ml)\n",
    "    \n",
    "#     training.set_memory_request('2G')\n",
    "#     training.set_cpu_request('1')\n",
    "\n",
    "Compiler().compile(training_pipeline, '.training.tar.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the pipeline\n",
    "\n",
    "Code below will run a pipeline and inject some pipeline parameters. Here we provide two versions of data sets\n",
    "* `SAMPLE_DATA_SET` - Data set that has just over 2 megabytes. Not enough for sufficient training. However ideal for development, because of faster feedback.\n",
    "* `FULL_DATA_SET` - Precreated data set with all github issues. 3 gigabytes. Good enough for sufficient model\n",
    "\n",
    "Depending on your needs you can choose one or another data set and pass it as a pipeline parameter `data-set`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# github issues small: 2Mi data set (best for dev/test)\n",
    "SAMPLE_DATASET = 'https://s3.us-east-2.amazonaws.com/asi-kubeflow-models/gh-issues/data-sample.csv'\n",
    "SAMPLE_DATASET_MD5 = '916af946f2fe1d1779b26205d4d8378f'\n",
    "# data set for 3Gi. (best for training)\n",
    "FULL_DATASET = 'https://s3.us-east-2.amazonaws.com/asi-kubeflow-models/gh-issues/data-full.csv'\n",
    "FULL_DATASET_MD5 = '57dc987c04d41a94d0d9daf4d0ebf8ba'\n",
    "\n",
    "run = client.run_pipeline(exp.id, f'Training model {TAG}: {datetime.now():%m%d-%H%M}', '.training.tar.gz',\n",
    "                          params={\n",
    "                              'import-from': SAMPLE_DATASET,\n",
    "                              'dataset-md5': SAMPLE_DATASET_MD5,\n",
    "                              'dataset-file': DATASET_FILE,\n",
    "                              'title-pp-file': TITLE_PP_FILE,\n",
    "                              'body-pp-file': BODY_PP_FILE,\n",
    "                              'train-df-file': TRAIN_DF_FILE,\n",
    "                              'test-df-file': TEST_DF_FILE,\n",
    "                              'train-title-vecs': TRAIN_TITLE_VECS,\n",
    "                              'train-body-vecs': TRAIN_BODY_VECS,\n",
    "                              'model-file': MODEL_FILE,\n",
    "                              'learning-rate': 0.001,\n",
    "                              'sample-size': 100,\n",
    "                          })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# block until job completion\n",
    "print(f\"Waiting for run: {run.id}...\")\n",
    "result = client.wait_for_run_completion(run.id, timeout=720).run.status\n",
    "print(f\"Finished with: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serving with Seldon\n",
    "Prepping a container for serving. \n",
    "\n",
    "Here we define all variables that will be needed for our dockerfile tempalte. \n",
    "\n",
    "- `MODEL_WRAPPER`: is a name of a python class that adapts keras model for serving\n",
    "- `MODEL_NAME`: used in seldon deployment\n",
    "- `MODEL_VERSION`: one model can be served multiple times with different versions simulteniously\n",
    "- `SELDON_DEPLOYMENT`: name of kubernetes resource\n",
    "- `SELDON_OAUTH_KEY`: part of shared secret between `SeldonDeployment` and a client application\n",
    "- `SELDON_OAUTH_SECRET`: part of shared secret between `SeldonDeployment` and a client application\n",
    "- `REPLICAS`: number of replicas for `SeldonDeployment` pod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "MODEL_WRAPPER = 'IssueSummarizationModel'\n",
    "MODEL_NAME = re.sub(r'\\W+', '-', MODEL_WRAPPER).lower()\n",
    "MODEL_VERSION = TAG\n",
    "SELDON_DEPLOYMENT = f\"{MODEL_NAME}-{MODEL_VERSION}\"\n",
    "# here we hash a information about model, so it would be predictable\n",
    "SELDON_OAUTH_KEY = utils.sha1(MODEL_NAME, MODEL_VERSION, NAMESPACE)\n",
    "# for secure secret we will use hash of user defined shared secret salted with OAUTH_KEY\n",
    "SELDON_OAUTH_SECRET = utils.sha1(SELDON_OAUTH_KEY, get_secret('USER_SECRET_FOR_MODEL'))\n",
    "SELDON_APISERVER_ADDR=f\"seldon-seldon-apiserver.{NAMESPACE}:8080\"\n",
    "\n",
    "SELDON_DEPLOYMENT_REPLICAS = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a serving image\n",
    "\n",
    "`SeldonDeployment` needs a docker image that contains a model wrapper written in (but not limited) Python\n",
    "\n",
    "This step will build a container and serve it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%template components/serving/Dockerfile\n",
    "FROM seldonio/seldon-core-s2i-python3\n",
    "\n",
    "FROM {{TRAINING_IMAGE}}\n",
    "RUN pip3 install --no-cache-dir -U 'seldon-core'\n",
    "\n",
    "COPY --from=0 /microservice /microservice\n",
    "COPY src/serving.py /microservice/{{MODEL_WRAPPER}}.py\n",
    "COPY src/seq2seq_utils.py /microservice\n",
    "COPY src/text_utils.py /microservice\n",
    "\n",
    "WORKDIR /microservice\n",
    "ENTRYPOINT [\"python\",\"-u\",\"microservice.py\"]\n",
    "CMD [\"{{MODEL_WRAPPER}}\", \"REST\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to serve trained model we build an image with our serving microservice. To achieve this we reuse our kaniko pipeline defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_to_s3(\n",
    "    destination=f\"s3://{BUCKET_NAME}/{BUILD_CONTEXT}\",\n",
    "    ignorefile='components/kaniko/ignorefile.txt',\n",
    "    workspace='.',\n",
    "    s3_client=s3,\n",
    ")\n",
    "\n",
    "run = client.run_pipeline(exp.id, f'Build image: serving:{TAG}', '.kaniko.tar.gz', \n",
    "                          params={\n",
    "                              'image': SERVING_IMAGE,\n",
    "                              'build-context': f\"{MOUNT_PATH}/{BUILD_CONTEXT}/components/serving\"\n",
    "                          })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# block until job completion\n",
    "print(f\"Waiting for run: {run.id}...\")\n",
    "result = client.wait_for_run_completion(run.id, timeout=720).run.status\n",
    "print(f\"Finished with: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serve model\n",
    "\n",
    "Then we render our `SeldonDeployment` template and deploy it with `kubectl`, similar as we have done before with `pvc` definition. Here we define reference to the model that will be used for serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%templatefile components/serving/templates/seldon.yaml -o seldon.yaml\n",
    "!kubectl apply -f seldon.yaml --wait\n",
    "!kubectl get -f seldon.yaml -o jsonpath='{.status.state}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate\n",
    "\n",
    "It can take few minutes while a seldon applicaiton will be deployed. Once it will be deployed. Then we can send a test prediction\n",
    "\n",
    "Test model serving by accessing seldon api server. Because Seldon API server provides an oauth, we need to receive a temporrary bearer token. We can receive this token by providing oauth key and secret that has been used in our `SeldonDeployment`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import Code\n",
    "import nbextensions.seldon as seldon\n",
    "\n",
    "test_payload = {\n",
    "    \"data\":{\"ndarray\": [[\"try to stop flask from using multiple threads\"]]},\n",
    "}\n",
    "                         \n",
    "t = seldon.get_token(\n",
    "    server=SELDON_APISERVER_ADDR,\n",
    "    oauth_key=SELDON_OAUTH_KEY,\n",
    "    oauth_secret=SELDON_OAUTH_SECRET,\n",
    ")\n",
    "result = seldon.prediction(\n",
    "    server=SELDON_APISERVER_ADDR,\n",
    "    payload=test_payload,\n",
    "    token=t,\n",
    ")\n",
    "if result.get('status') == 'FAILURE':\n",
    "    print(\"Error connecting to seldon core.\", \n",
    "          \"This may happen when Seldon has not been up and running yet.\",\n",
    "          \"Try again later\")\n",
    "    display(Code(f\"{result.get('reason')}: {result.get('info')}\"))\n",
    "else:\n",
    "    display(\n",
    "        pd.DataFrame.from_dict({\n",
    "            'test': test_payload['data']['ndarray'][0],\n",
    "            'prediction': result['data']['ndarray'][0],\n",
    "    }))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy a client application\n",
    "\n",
    "This section will be focused on application deployment routines.\n",
    "- `FLASK_APP`: name of the kubernetes deployment associated with the applicaiton\n",
    "- `FLASK_REPLICAS`: number of replicas for application deployment pod\n",
    "- `GITHUB_TOKEN`: Github Token to access Github API. This will help application to fetch a random github issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLASK_APP=APPLICATION_NAME\n",
    "FLASK_REPLICAS = 1\n",
    "GITHUB_TOKEN=get_secret('github_token')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building an application container\n",
    "\n",
    "User application has been implemented inside [app.py](components/flaskapp/src/app.py). We bake this applicaiton inside of docker container and deploy it further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%template components/flaskapp/Dockerfile -v\n",
    "FROM {{TRAINING_IMAGE}}\n",
    "RUN pip3 install --no-cache-dir -U 'flask>=0.12.3'\n",
    "COPY src/ /app\n",
    "WORKDIR /app\n",
    "ENTRYPOINT [\"python\",\"-u\",\"app.py\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_to_s3(\n",
    "    destination=f\"s3://{BUCKET_NAME}/{BUILD_CONTEXT}\",\n",
    "    ignorefile='components/kaniko/ignorefile.txt',\n",
    "    workspace='.',\n",
    "    s3_client=s3,\n",
    ")\n",
    "\n",
    "run = client.run_pipeline(\n",
    "    exp.id, f'Build image: application:{TAG}', '.kaniko.tar.gz', \n",
    "    params={\n",
    "      'image': FLASK_APP_IMAGE,\n",
    "      'build-context': f\"{MOUNT_PATH}/{BUILD_CONTEXT}/components/flaskapp\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# block until job completion\n",
    "print(f\"Waiting for run: {run.id}...\")\n",
    "result = client.wait_for_run_completion(run.id, timeout=720).run.status\n",
    "print(f\"Finished with: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy a web application\n",
    "\n",
    "Client web application is a simple Python [flask](http://flask.pocoo.org) application. Deployment manifest can be defined via kubernetes deployment template file [link](components/flaskapp/templates/application.yaml). We render this template with current notebook global variables and then use `kubectl` to deploy.\n",
    "\n",
    "For web access application deployment will use an Ambassador http router, which is part of Kubeflow stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%templatefile components/flaskapp/templates/application.yaml -o application.yaml\n",
    "!kubectl apply -f application.yaml --wait\n",
    "\n",
    "from IPython.display import Markdown, HTML\n",
    "display(HTML(f'Application can be accessible <a href=\"/{FLASK_APP}/\" target=\"_blank\">here</a>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tear down\n",
    "\n",
    "Uppon completion, let's tear everything down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kubectl delete -f seldon.yaml\n",
    "!kubectl delete -f application.yaml"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "336px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 201,
   "position": {
    "height": "223px",
    "left": "1208px",
    "right": "20px",
    "top": "262px",
    "width": "400px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
