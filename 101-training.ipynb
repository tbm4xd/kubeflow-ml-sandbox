{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Initialization\" data-toc-modified-id=\"Initialization-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Initialization</a></span><ul class=\"toc-item\"><li><span><a href=\"#Define-variables-for-experiment\" data-toc-modified-id=\"Define-variables-for-experiment-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Define variables for experiment</a></span></li><li><span><a href=\"#Download-data-set\" data-toc-modified-id=\"Download-data-set-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Download data set</a></span></li></ul></li><li><span><a href=\"#Process-Data\" data-toc-modified-id=\"Process-Data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Process Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Split-data\" data-toc-modified-id=\"Split-data-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Split data</a></span></li><li><span><a href=\"#Preprocess-data-for-Machine-Learning\" data-toc-modified-id=\"Preprocess-data-for-Machine-Learning-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Preprocess data for Machine Learning</a></span></li></ul></li><li><span><a href=\"#Training\" data-toc-modified-id=\"Training-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Training</a></span></li><li><span><a href=\"#See-prediction\" data-toc-modified-id=\"See-prediction-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>See prediction</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 101 Github Issue Summarization\n",
    "\n",
    "\n",
    "In this notebook, we will show how to:\n",
    "\n",
    "* Define a seq2seq model\n",
    "* Perform training with [Keras](https://keras.io) and [Tensorflow](https://www.tensorflow.org/api_docs/python/tf)\n",
    "* Validate model [Seldon](https://docs.seldon.io/projects/seldon-core/en/latest/python/api/modules.html)\n",
    "\n",
    "To perform the training we have wired few technologies together. \n",
    "* **S3 bucket** as mounted a file system [here](../bucket): this is place for all training artifacts. Keras prefers to work with the artifacts as it would be normal files.\n",
    "* **Notebook profile** [here](profile_default/startup) contains meaningful notebook defaults and settings\n",
    "* **SuperHub integration** contains various environment provisioning scripts \n",
    "* **Git integration** your notebook has a git repository. It is wise to make periodical commits into the git. Best way to do it is to use Jupyter terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from os import environ, makedirs\n",
    "from nbextensions.utils import download_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define variables for experiment\n",
    "In the beginning of the scrip we define all necessary variables. This is a good start, we have a single cell to define all experiment configuration in one place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAG = 'latest'\n",
    "\n",
    "ARTIFACTS_ROOT = f\"{environ['HOME']}/data/training-{TAG}\"\n",
    "DATASET_FILE = f\"{ARTIFACTS_ROOT}/dataset.csv\"\n",
    "MODEL_FILE = f\"{ARTIFACTS_ROOT}/training1.h5\"\n",
    "TITLE_PP_FILE = f\"{ARTIFACTS_ROOT}/title_preprocessor.dpkl\"\n",
    "BODY_PP_FILE = f\"{ARTIFACTS_ROOT}/body_preprocessor.dpkl\"\n",
    "TRAIN_DF_FILE = f\"{ARTIFACTS_ROOT}/traindf.csv\"\n",
    "TEST_DF_FILE =  f\"{ARTIFACTS_ROOT}/testdf.csv\"\n",
    "TRAIN_TITLE_VECS = f\"{ARTIFACTS_ROOT}/train_title_vecs.npy\"\n",
    "TRAIN_BODY_VECS = f\"{ARTIFACTS_ROOT}/train_body_vecs.npy\"\n",
    "\n",
    "TRAINING_DATA_SIZE = 2000\n",
    "TEST_SIZE = .10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data set \n",
    "\n",
    "Before we start training we need to download a dataset file in a CSV format. \n",
    "\n",
    "Here we have two data set file. You can choose eather of them eather of them:\n",
    "- *2Mi* `sample-dataset`: This is good for tryout and debug your preprocessing or testing python scripts because of fast turnover. However model trained on such small dataset will certainly not be very accurate\n",
    "- *3Gi* `full-dataset`: This dataset takes significant time for training, however predictions based on this model are quite good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# github issues small: 2Mi data set (best for dev/test)\n",
    "SAMPLE_DATASET = 'https://s3.us-east-2.amazonaws.com/asi-kubeflow-models/gh-issues/data-sample.csv'\n",
    "SAMPLE_DATASET_MD5 = '916af946f2fe1d1779b26205d4d8378f'\n",
    "# data set for 3Gi. (best for training)\n",
    "FULL_DATASET = 'https://s3.us-east-2.amazonaws.com/asi-kubeflow-models/gh-issues/data-full.csv'\n",
    "FULL_DATASET_MD5 = '57dc987c04d41a94d0d9daf4d0ebf8ba'\n",
    "\n",
    "%time\n",
    "download_file(\n",
    "    url=SAMPLE_DATASET, \n",
    "    md5sum=SAMPLE_DATASET_MD5, \n",
    "    download_to=DATASET_FILE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data\n",
    "Before we process data for machine learning, we need to split data into training and test data sets (variable `TEST_SIZE`). To accelerate a training we can also limit data size (variable `TRAINING_DATA_SIZE`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "if TRAINING_DATA_SIZE:\n",
    "    traindf, testdf = train_test_split(pd.read_csv(DATASET_FILE).sample(n=TRAINING_DATA_SIZE), test_size=TEST_SIZE)\n",
    "else:\n",
    "    traindf, testdf = train_test_split(pd.read_csv(DATASET_FILE),test_size=TEST_SIZE)\n",
    "\n",
    "print(f'Train: {traindf.shape[0]:,} rows {traindf.shape[1]:,} columns')\n",
    "print(f'Test: {testdf.shape[0]:,} rows {testdf.shape[1]:,} columns')\n",
    "\n",
    "# preview data\n",
    "traindf.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data for Machine Learning\n",
    "Here we will use `ktext` library [documentation](https://github.com/hamelsmu/ktext)\n",
    "\n",
    "We will convert text into a vector and do the same for title and body:\n",
    "\n",
    "* **body**: Clean, tokenize, and apply padding / truncating such that each document `length = 70` also, retain only the top `8,000` words in the vocabulary and set the remaining words to 1 which will become common index for rare words\n",
    "\n",
    "* **title**: Instantiate a text processor for the titles, with some different parameters `append_indicators=True` appends the tokens `_start_` and `_end_` to each document. `padding='post'` means that zero padding is appended to the end of the of the document (as opposed to the default which is 'pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "from ktext.preprocess import processor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "import dill as dpickle\n",
    "import numpy as np\n",
    "\n",
    "train_body_raw = traindf.body.tolist()\n",
    "body_pp = processor(keep_n=8000, padding_maxlen=70)\n",
    "train_body_vecs = body_pp.fit_transform(train_body_raw)\n",
    "\n",
    "train_title_raw = traindf.issue_title.tolist()\n",
    "title_pp = processor(append_indicators=True, keep_n=4500, padding_maxlen=12, padding ='post')\n",
    "train_title_vecs = title_pp.fit_transform(train_title_raw)\n",
    "\n",
    "# preview\n",
    "data = np.array([['Before', train_title_raw[0], train_body_raw[0]],\n",
    "                ['After', train_title_vecs[0], train_body_vecs[0]]])\n",
    "df = pd.DataFrame(data=data, columns=['', 'Issue Title', 'Issue body'])\n",
    "display(HTML(df.to_html(index=False)))\n",
    "\n",
    "# Save the preprocessor\n",
    "print(f\"Saving {BODY_PP_FILE}\")\n",
    "with open(BODY_PP_FILE, 'wb') as f:\n",
    "    dpickle.dump(body_pp, f)\n",
    "\n",
    "print(f\"Saving {TITLE_PP_FILE}\")\n",
    "with open(TITLE_PP_FILE, 'wb') as f:\n",
    "    dpickle.dump(title_pp, f)\n",
    "\n",
    "# Save the processed data\n",
    "print(f\"Saving {TRAIN_TITLE_VECS}\")\n",
    "np.save(TRAIN_TITLE_VECS, train_title_vecs)\n",
    "print(f\"Saving {TRAIN_BODY_VECS}\")\n",
    "np.save(TRAIN_BODY_VECS, train_body_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Now we are ready to start our training. Training has been implemented as a [python script](components/training/src/train.py). It takes following variables defined in the notebook user space (above) as the implicit input\n",
    "* `TITLE_PP_FILE`\n",
    "* `BODY_PP_FILE`\n",
    "* `TRAIN_DF_FILE`\n",
    "* `TEST_DF_FILE`\n",
    "* `MODEL_FILE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "%run 'components/training/src/train.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See prediction\n",
    "It is useful to see examples of real predictions on a holdout set to get a sense of the performance of the model. We will also evaluate the model numerically in a following section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "from keras.models import load_model\n",
    "from seq2seq_utils import Seq2Seq_Inference\n",
    "seq2seq_Model = load_model(MODEL_FILE)\n",
    "seq2seq_inf = Seq2Seq_Inference(encoder_preprocessor=body_pp,\n",
    "                                 decoder_preprocessor=title_pp,\n",
    "                                 seq2seq_model=seq2seq_Model)\n",
    "seq2seq_inf.demo_model_predictions(n=1, issue_df=testdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
